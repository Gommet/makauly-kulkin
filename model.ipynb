{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import data\n",
    "import model as model_mod\n",
    "from torch import nn\n",
    "from torch import Tensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(data)\n",
    "dataset = data.CustomDataset('clean.pickle', 'Bakery', 1, 0.7, True)\n",
    "train_loader = DataLoader(dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss epoch 0:  4095841.0\n",
      "Loss epoch 1:  4095080.0\n",
      "Loss epoch 2:  4094363.25\n",
      "Loss epoch 3:  4093716.75\n",
      "Loss epoch 4:  4093147.25\n",
      "Loss epoch 5:  4092637.0\n",
      "Loss epoch 6:  4092166.25\n",
      "Loss epoch 7:  4091723.0\n",
      "Loss epoch 8:  4091298.75\n",
      "Loss epoch 9:  4090888.25\n",
      "Loss epoch 10:  4090487.75\n",
      "Loss epoch 11:  4090095.25\n",
      "Loss epoch 12:  4089708.5\n",
      "Loss epoch 13:  4089327.25\n",
      "Loss epoch 14:  4088949.75\n",
      "Loss epoch 15:  4088576.0\n",
      "Loss epoch 16:  4088206.0\n",
      "Loss epoch 17:  4087838.5\n",
      "Loss epoch 18:  4087473.25\n",
      "Loss epoch 19:  4087110.5\n",
      "Loss epoch 20:  4086749.75\n",
      "Loss epoch 21:  4086390.5\n",
      "Loss epoch 22:  4086032.75\n",
      "Loss epoch 23:  4085677.5\n",
      "Loss epoch 24:  4085322.75\n",
      "Loss epoch 25:  4084969.0\n",
      "Loss epoch 26:  4084617.25\n",
      "Loss epoch 27:  4084266.0\n",
      "Loss epoch 28:  4083916.25\n",
      "Loss epoch 29:  4083567.0\n",
      "Loss epoch 30:  4083218.5\n",
      "Loss epoch 31:  4082871.0\n",
      "Loss epoch 32:  4082524.5\n",
      "Loss epoch 33:  4082178.75\n",
      "Loss epoch 34:  4081832.5\n",
      "Loss epoch 35:  4081488.5\n",
      "Loss epoch 36:  4081144.75\n",
      "Loss epoch 37:  4080801.0\n",
      "Loss epoch 38:  4080458.5\n",
      "Loss epoch 39:  4080115.5\n",
      "Loss epoch 40:  4079774.5\n",
      "Loss epoch 41:  4079432.75\n",
      "Loss epoch 42:  4079091.5\n",
      "Loss epoch 43:  4078751.5\n",
      "Loss epoch 44:  4078411.25\n",
      "Loss epoch 45:  4078071.5\n",
      "Loss epoch 46:  4077732.5\n",
      "Loss epoch 47:  4077393.25\n",
      "Loss epoch 48:  4077054.75\n",
      "Loss epoch 49:  4076716.75\n",
      "Loss epoch 50:  4076379.0\n",
      "Loss epoch 51:  4076040.75\n",
      "Loss epoch 52:  4075704.0\n",
      "Loss epoch 53:  4075366.75\n",
      "Loss epoch 54:  4075030.0\n",
      "Loss epoch 55:  4074693.75\n",
      "Loss epoch 56:  4074357.25\n",
      "Loss epoch 57:  4074021.25\n",
      "Loss epoch 58:  4073685.75\n",
      "Loss epoch 59:  4073350.5\n",
      "Loss epoch 60:  4073015.75\n",
      "Loss epoch 61:  4072680.25\n",
      "Loss epoch 62:  4072345.5\n",
      "Loss epoch 63:  4072011.25\n",
      "Loss epoch 64:  4071676.75\n",
      "Loss epoch 65:  4071342.0\n",
      "Loss epoch 66:  4071008.5\n",
      "Loss epoch 67:  4070674.75\n",
      "Loss epoch 68:  4070340.75\n",
      "Loss epoch 69:  4070007.25\n",
      "Loss epoch 70:  4069674.25\n",
      "Loss epoch 71:  4069341.5\n",
      "Loss epoch 72:  4069008.25\n",
      "Loss epoch 73:  4068675.5\n",
      "Loss epoch 74:  4068343.25\n",
      "Loss epoch 75:  4068010.25\n",
      "Loss epoch 76:  4067678.25\n",
      "Loss epoch 77:  4067346.5\n",
      "Loss epoch 78:  4067014.0\n",
      "Loss epoch 79:  4066682.25\n",
      "Loss epoch 80:  4066350.25\n",
      "Loss epoch 81:  4066019.0\n",
      "Loss epoch 82:  4065687.75\n",
      "Loss epoch 83:  4065356.25\n",
      "Loss epoch 84:  4065025.5\n",
      "Loss epoch 85:  4064694.5\n",
      "Loss epoch 86:  4064363.25\n",
      "Loss epoch 87:  4064033.0\n",
      "Loss epoch 88:  4063702.0\n",
      "Loss epoch 89:  4063371.25\n",
      "Loss epoch 90:  4063041.0\n",
      "Loss epoch 91:  4062710.5\n",
      "Loss epoch 92:  4062380.25\n",
      "Loss epoch 93:  4062050.5\n",
      "Loss epoch 94:  4061720.75\n",
      "Loss epoch 95:  4061391.0\n",
      "Loss epoch 96:  4061061.25\n",
      "Loss epoch 97:  4060731.0\n",
      "Loss epoch 98:  4060401.5\n",
      "Loss epoch 99:  4060072.0\n",
      "Loss epoch 100:  4059743.25\n",
      "Loss epoch 101:  4059413.25\n",
      "Loss epoch 102:  4059084.75\n",
      "Loss epoch 103:  4058755.0\n",
      "Loss epoch 104:  4058425.75\n",
      "Loss epoch 105:  4058097.25\n",
      "Loss epoch 106:  4057768.75\n",
      "Loss epoch 107:  4057440.0\n",
      "Loss epoch 108:  4057111.5\n",
      "Loss epoch 109:  4056783.0\n",
      "Loss epoch 110:  4056454.5\n",
      "Loss epoch 111:  4056126.25\n",
      "Loss epoch 112:  4055797.75\n",
      "Loss epoch 113:  4055469.25\n",
      "Loss epoch 114:  4055141.5\n",
      "Loss epoch 115:  4054814.0\n",
      "Loss epoch 116:  4054485.5\n",
      "Loss epoch 117:  4054157.75\n",
      "Loss epoch 118:  4053830.5\n",
      "Loss epoch 119:  4053502.25\n",
      "Loss epoch 120:  4053174.5\n",
      "Loss epoch 121:  4052847.0\n",
      "Loss epoch 122:  4052519.5\n",
      "Loss epoch 123:  4052192.5\n",
      "Loss epoch 124:  4051865.0\n",
      "Loss epoch 125:  4051537.75\n",
      "Loss epoch 126:  4051210.75\n",
      "Loss epoch 127:  4050884.25\n",
      "Loss epoch 128:  4050556.75\n",
      "Loss epoch 129:  4050229.25\n",
      "Loss epoch 130:  4049902.25\n",
      "Loss epoch 131:  4049576.0\n",
      "Loss epoch 132:  4049248.75\n",
      "Loss epoch 133:  4048922.25\n",
      "Loss epoch 134:  4048595.75\n",
      "Loss epoch 135:  4048269.5\n",
      "Loss epoch 136:  4047942.5\n",
      "Loss epoch 137:  4047615.75\n",
      "Loss epoch 138:  4047289.75\n",
      "Loss epoch 139:  4046963.25\n",
      "Loss epoch 140:  4046637.25\n",
      "Loss epoch 141:  4046311.25\n",
      "Loss epoch 142:  4045985.5\n",
      "Loss epoch 143:  4045659.25\n",
      "Loss epoch 144:  4045333.0\n",
      "Loss epoch 145:  4045007.5\n",
      "Loss epoch 146:  4044681.25\n",
      "Loss epoch 147:  4044355.0\n",
      "Loss epoch 148:  4044030.25\n",
      "Loss epoch 149:  4043704.0\n",
      "Loss epoch 150:  4043378.5\n",
      "Loss epoch 151:  4043052.75\n",
      "Loss epoch 152:  4042727.25\n",
      "Loss epoch 153:  4042402.0\n",
      "Loss epoch 154:  4042077.0\n",
      "Loss epoch 155:  4041751.25\n",
      "Loss epoch 156:  4041426.0\n",
      "Loss epoch 157:  4041100.75\n",
      "Loss epoch 158:  4040775.75\n",
      "Loss epoch 159:  4040450.5\n",
      "Loss epoch 160:  4040125.75\n",
      "Loss epoch 161:  4039800.5\n",
      "Loss epoch 162:  4039475.5\n",
      "Loss epoch 163:  4039150.25\n",
      "Loss epoch 164:  4038826.0\n",
      "Loss epoch 165:  4038501.25\n",
      "Loss epoch 166:  4038176.5\n",
      "Loss epoch 167:  4037851.5\n",
      "Loss epoch 168:  4037527.25\n",
      "Loss epoch 169:  4037202.5\n",
      "Loss epoch 170:  4036878.25\n",
      "Loss epoch 171:  4036553.5\n",
      "Loss epoch 172:  4036229.5\n",
      "Loss epoch 173:  4035905.0\n",
      "Loss epoch 174:  4035581.0\n",
      "Loss epoch 175:  4035256.25\n",
      "Loss epoch 176:  4034932.25\n",
      "Loss epoch 177:  4034608.0\n",
      "Loss epoch 178:  4034283.75\n",
      "Loss epoch 179:  4033959.5\n",
      "Loss epoch 180:  4033635.75\n",
      "Loss epoch 181:  4033312.25\n",
      "Loss epoch 182:  4032988.25\n",
      "Loss epoch 183:  4032664.25\n",
      "Loss epoch 184:  4032340.75\n",
      "Loss epoch 185:  4032016.5\n",
      "Loss epoch 186:  4031693.0\n",
      "Loss epoch 187:  4031369.0\n",
      "Loss epoch 188:  4031045.75\n",
      "Loss epoch 189:  4030722.0\n",
      "Loss epoch 190:  4030398.5\n",
      "Loss epoch 191:  4030074.75\n",
      "Loss epoch 192:  4029751.5\n",
      "Loss epoch 193:  4029428.5\n",
      "Loss epoch 194:  4029104.5\n",
      "Loss epoch 195:  4028781.75\n",
      "Loss epoch 196:  4028458.25\n",
      "Loss epoch 197:  4028135.0\n",
      "Loss epoch 198:  4027812.25\n",
      "Loss epoch 199:  4027488.5\n",
      "Loss epoch 200:  4027166.0\n",
      "Loss epoch 201:  4026842.25\n",
      "Loss epoch 202:  4026519.5\n",
      "Loss epoch 203:  4026196.75\n",
      "Loss epoch 204:  4025873.5\n",
      "Loss epoch 205:  4025551.25\n",
      "Loss epoch 206:  4025227.25\n",
      "Loss epoch 207:  4024905.25\n",
      "Loss epoch 208:  4024582.5\n",
      "Loss epoch 209:  4024260.25\n",
      "Loss epoch 210:  4023937.75\n",
      "Loss epoch 211:  4023614.75\n",
      "Loss epoch 212:  4023291.75\n",
      "Loss epoch 213:  4022969.75\n",
      "Loss epoch 214:  4022647.25\n",
      "Loss epoch 215:  4022324.75\n",
      "Loss epoch 216:  4022002.5\n",
      "Loss epoch 217:  4021679.75\n",
      "Loss epoch 218:  4021357.5\n",
      "Loss epoch 219:  4021035.25\n",
      "Loss epoch 220:  4020713.75\n",
      "Loss epoch 221:  4020391.5\n",
      "Loss epoch 222:  4020069.75\n",
      "Loss epoch 223:  4019747.0\n",
      "Loss epoch 224:  4019424.5\n",
      "Loss epoch 225:  4019103.25\n",
      "Loss epoch 226:  4018781.0\n",
      "Loss epoch 227:  4018459.0\n",
      "Loss epoch 228:  4018137.25\n",
      "Loss epoch 229:  4017815.5\n",
      "Loss epoch 230:  4017493.5\n",
      "Loss epoch 231:  4017171.75\n",
      "Loss epoch 232:  4016850.0\n",
      "Loss epoch 233:  4016528.75\n",
      "Loss epoch 234:  4016206.75\n",
      "Loss epoch 235:  4015885.25\n",
      "Loss epoch 236:  4015563.25\n",
      "Loss epoch 237:  4015242.25\n",
      "Loss epoch 238:  4014920.75\n",
      "Loss epoch 239:  4014599.0\n",
      "Loss epoch 240:  4014278.0\n",
      "Loss epoch 241:  4013956.5\n",
      "Loss epoch 242:  4013635.5\n",
      "Loss epoch 243:  4013313.75\n",
      "Loss epoch 244:  4012992.5\n",
      "Loss epoch 245:  4012671.5\n",
      "Loss epoch 246:  4012350.25\n",
      "Loss epoch 247:  4012029.25\n",
      "Loss epoch 248:  4011707.75\n",
      "Loss epoch 249:  4011386.75\n",
      "Loss epoch 250:  4011065.75\n",
      "Loss epoch 251:  4010744.5\n",
      "Loss epoch 252:  4010424.0\n",
      "Loss epoch 253:  4010102.75\n",
      "Loss epoch 254:  4009782.25\n",
      "Loss epoch 255:  4009461.5\n",
      "Loss epoch 256:  4009140.75\n",
      "Loss epoch 257:  4008820.25\n",
      "Loss epoch 258:  4008499.25\n",
      "Loss epoch 259:  4008178.0\n",
      "Loss epoch 260:  4007858.0\n",
      "Loss epoch 261:  4007537.5\n",
      "Loss epoch 262:  4007217.0\n",
      "Loss epoch 263:  4006896.0\n",
      "Loss epoch 264:  4006575.75\n",
      "Loss epoch 265:  4006255.75\n",
      "Loss epoch 266:  4005934.5\n",
      "Loss epoch 267:  4005614.75\n",
      "Loss epoch 268:  4005294.75\n",
      "Loss epoch 269:  4004974.0\n",
      "Loss epoch 270:  4004654.0\n",
      "Loss epoch 271:  4004333.25\n",
      "Loss epoch 272:  4004013.0\n",
      "Loss epoch 273:  4003693.0\n",
      "Loss epoch 274:  4003373.0\n",
      "Loss epoch 275:  4003053.0\n",
      "Loss epoch 276:  4002733.0\n",
      "Loss epoch 277:  4002412.25\n",
      "Loss epoch 278:  4002092.75\n",
      "Loss epoch 279:  4001772.75\n",
      "Loss epoch 280:  4001453.0\n",
      "Loss epoch 281:  4001132.75\n",
      "Loss epoch 282:  4000813.25\n",
      "Loss epoch 283:  4000493.5\n",
      "Loss epoch 284:  4000173.75\n",
      "Loss epoch 285:  3999854.0\n",
      "Loss epoch 286:  3999534.5\n",
      "Loss epoch 287:  3999214.5\n",
      "Loss epoch 288:  3998894.75\n",
      "Loss epoch 289:  3998575.25\n",
      "Loss epoch 290:  3998255.5\n",
      "Loss epoch 291:  3997936.0\n",
      "Loss epoch 292:  3997616.5\n",
      "Loss epoch 293:  3997297.0\n",
      "Loss epoch 294:  3996977.25\n",
      "Loss epoch 295:  3996658.0\n",
      "Loss epoch 296:  3996338.5\n",
      "Loss epoch 297:  3996019.25\n",
      "Loss epoch 298:  3995700.5\n",
      "Loss epoch 299:  3995381.25\n",
      "Loss epoch 300:  3995062.0\n",
      "Loss epoch 301:  3994742.75\n",
      "Loss epoch 302:  3994423.25\n",
      "Loss epoch 303:  3994104.5\n",
      "Loss epoch 304:  3993785.75\n",
      "Loss epoch 305:  3993466.5\n",
      "Loss epoch 306:  3993147.25\n",
      "Loss epoch 307:  3992828.25\n",
      "Loss epoch 308:  3992509.75\n",
      "Loss epoch 309:  3992190.25\n",
      "Loss epoch 310:  3991871.5\n",
      "Loss epoch 311:  3991552.75\n",
      "Loss epoch 312:  3991233.75\n",
      "Loss epoch 313:  3990914.75\n",
      "Loss epoch 314:  3990596.5\n",
      "Loss epoch 315:  3990277.75\n",
      "Loss epoch 316:  3989959.25\n",
      "Loss epoch 317:  3989640.25\n",
      "Loss epoch 318:  3989322.0\n",
      "Loss epoch 319:  3989002.75\n",
      "Loss epoch 320:  3988684.75\n",
      "Loss epoch 321:  3988366.0\n",
      "Loss epoch 322:  3988047.75\n",
      "Loss epoch 323:  3987729.0\n",
      "Loss epoch 324:  3987410.75\n",
      "Loss epoch 325:  3987092.25\n",
      "Loss epoch 326:  3986774.25\n",
      "Loss epoch 327:  3986455.25\n",
      "Loss epoch 328:  3986137.25\n",
      "Loss epoch 329:  3985819.25\n",
      "Loss epoch 330:  3985501.25\n",
      "Loss epoch 331:  3985182.5\n",
      "Loss epoch 332:  3984864.25\n",
      "Loss epoch 333:  3984546.75\n",
      "Loss epoch 334:  3984228.5\n",
      "Loss epoch 335:  3983910.5\n",
      "Loss epoch 336:  3983591.75\n",
      "Loss epoch 337:  3983274.0\n",
      "Loss epoch 338:  3982956.5\n",
      "Loss epoch 339:  3982638.0\n",
      "Loss epoch 340:  3982320.0\n",
      "Loss epoch 341:  3982002.25\n",
      "Loss epoch 342:  3981684.75\n",
      "Loss epoch 343:  3981366.25\n",
      "Loss epoch 344:  3981048.75\n",
      "Loss epoch 345:  3980731.25\n",
      "Loss epoch 346:  3980414.0\n",
      "Loss epoch 347:  3980095.75\n",
      "Loss epoch 348:  3979778.75\n",
      "Loss epoch 349:  3979461.0\n",
      "Loss epoch 350:  3979143.75\n",
      "Loss epoch 351:  3978825.5\n",
      "Loss epoch 352:  3978507.75\n",
      "Loss epoch 353:  3978190.5\n",
      "Loss epoch 354:  3977873.25\n",
      "Loss epoch 355:  3977555.5\n",
      "Loss epoch 356:  3977238.5\n",
      "Loss epoch 357:  3976920.5\n",
      "Loss epoch 358:  3976603.75\n",
      "Loss epoch 359:  3976286.25\n",
      "Loss epoch 360:  3975968.5\n",
      "Loss epoch 361:  3975652.25\n",
      "Loss epoch 362:  3975334.5\n",
      "Loss epoch 363:  3975016.75\n",
      "Loss epoch 364:  3974700.5\n",
      "Loss epoch 365:  3974383.0\n",
      "Loss epoch 366:  3974066.25\n",
      "Loss epoch 367:  3973749.0\n",
      "Loss epoch 368:  3973432.0\n",
      "Loss epoch 369:  3973114.75\n",
      "Loss epoch 370:  3972798.25\n",
      "Loss epoch 371:  3972481.25\n",
      "Loss epoch 372:  3972164.5\n",
      "Loss epoch 373:  3971847.25\n",
      "Loss epoch 374:  3971530.25\n",
      "Loss epoch 375:  3971213.75\n",
      "Loss epoch 376:  3970896.75\n",
      "Loss epoch 377:  3970579.75\n",
      "Loss epoch 378:  3970263.5\n",
      "Loss epoch 379:  3969946.75\n",
      "Loss epoch 380:  3969630.25\n",
      "Loss epoch 381:  3969313.75\n",
      "Loss epoch 382:  3968996.75\n",
      "Loss epoch 383:  3968680.25\n",
      "Loss epoch 384:  3968363.75\n",
      "Loss epoch 385:  3968047.5\n",
      "Loss epoch 386:  3967730.75\n",
      "Loss epoch 387:  3967414.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ori/uni/hackupc/kriker/model.ipynb Cell 3'\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ori/uni/hackupc/kriker/model.ipynb#ch0000002?line=21'>22</a>\u001b[0m     loss \u001b[39m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ori/uni/hackupc/kriker/model.ipynb#ch0000002?line=22'>23</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ori/uni/hackupc/kriker/model.ipynb#ch0000002?line=23'>24</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ori/uni/hackupc/kriker/model.ipynb#ch0000002?line=24'>25</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ori/uni/hackupc/kriker/model.ipynb#ch0000002?line=25'>26</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mLoss epoch \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m: \u001b[39m\u001b[39m\"\u001b[39m, loss\u001b[39m.\u001b[39mitem())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/_tensor.py:363\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=353'>354</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=354'>355</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=355'>356</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=356'>357</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=360'>361</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=361'>362</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/_tensor.py?line=362'>363</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=167'>168</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=169'>170</a>\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=170'>171</a>\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=171'>172</a>\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=172'>173</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=173'>174</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/ori/.local/lib/python3.10/site-packages/torch/autograd/__init__.py?line=174'>175</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "importlib.reload(model_mod)\n",
    "model = model_mod.RNN(2, 1, 2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "start_epoch = 0\n",
    "loss_final = float(\"inf\")\n",
    "try:\n",
    "    checkpoint = torch.load('model.pth')\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    loss_final = checkpoint['loss']\n",
    "except FileNotFoundError:\n",
    "    start_epoch = 0\n",
    "    loss_final = float(\"inf\")\n",
    "epochs = 500\n",
    "for e in range(start_epoch, epochs):\n",
    "    hidden = None\n",
    "    for x, y in train_loader:\n",
    "        pred, hidden = model(x, hidden)\n",
    "        hidden = hidden.data\n",
    "        loss = criterion(pred, y)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Loss epoch {e}: \", loss.item())\n",
    "    if loss_final > loss.item():\n",
    "        torch.save({\n",
    "            'epoch': e,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': loss,\n",
    "            }, 'model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
